{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96338489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002,2009,2016,2023,2037,2044,2051,2058,2072,2079,2086,2093,2107,2114,2121,2128,2142,2149,2156,2163,2177,2184,2191,2198,2212,2219,2226,2233,2247,2254,2261,2268,2282,2289,2296,2303,2317,2324,2331,2338,2352,2359,2366,2373,2387,2394,2401,2408,2422,2429,2436,2443,2457,2464,2471,2478,2492,2499,2506,2513,2527,2534,2541,2548,2562,2569,2576,2583,2597,2604,2611,2618,2632,2639,2646,2653,2667,2674,2681,2688,2702,2709,2716,2723,2737,2744,2751,2758,2772,2779,2786,2793,2807,2814,2821,2828,2842,2849,2856,2863,2877,2884,2891,2898,2912,2919,2926,2933,2947,2954,2961,2968,2982,2989,2996,3003,3017,3024,3031,3038,3052,3059,3066,3073,3087,3094,3101,3108,3122,3129,3136,3143,3157,3164,3171,3178,3192,3199\n"
     ]
    }
   ],
   "source": [
    "numbers = []\n",
    "\n",
    "for num in range(2000, 3201):\n",
    "    if num % 7 == 0 and num % 5 != 0:\n",
    "        numbers.append(str(num))\n",
    "\n",
    "print(','.join(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985b3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: I am Yash Gupta\n",
      "Number of Uppercase letters: 3\n",
      "Number of Lowercase letters: 9\n"
     ]
    }
   ],
   "source": [
    "def count_upper_lower(sentence):\n",
    "    upper_count = 0\n",
    "    lower_count = 0\n",
    "\n",
    "    for char in sentence:\n",
    "        if char.isupper():\n",
    "            upper_count += 1\n",
    "        elif char.islower():\n",
    "            lower_count += 1\n",
    "\n",
    "    return upper_count, lower_count\n",
    "\n",
    "\n",
    "input_sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "\n",
    "upper, lower = count_upper_lower(input_sentence)\n",
    "\n",
    "print(\"Number of Uppercase letters:\", upper)\n",
    "print(\"Number of Lowercase letters:\", lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans3."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e126ecc4",
   "metadata": {},
   "source": [
    "Analyze the nature of the data, understand the problem at hand, and evaluate the impact on model performance using Feature Selection and dimension reduction technique  named Principal Component Analysis.\n",
    "\n",
    "Firstly to normalize the data to ensure that features with different scales do not dominate the principal components.\n",
    "PCA aims to transform a dataset with potentially correlated features into a new set of uncorrelated features called principal components. It retains as much variance in the data as possible.\n",
    "Normalized data is feeded for PCA, \n",
    "The first principal component captures the most variance, followed by the second, and so on.\n",
    "PCA involves calculating the eigenvalues and eigenvectors of the covariance matrix of the original data. The eigenvectors form the principal components, and the eigenvalues indicate the amount of variance they capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans4. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d578ec2",
   "metadata": {},
   "source": [
    "Various factors that why we might not be entirely satisfied with the model's high 96% accuracy performance :\n",
    "\n",
    "1. Class Imbalance : If the dataset has a significant class imbalance (e.g., more samples of one class than the other), achieving high accuracy might not be indicative of the model's true performance. The model could be biased towards the majority class.\n",
    "\n",
    "2. Misleading Accuracy : Accuracy alone may not be an appropriate metric for imbalanced dataset. For example, if 95% of the samples belong to the negative class (non-cancer), a model that predicts all samples as negative would still achieve 95% accuracy. Therefore, it's crucial to consider other metrics and assess the model's performance on each class separately.\n",
    "\n",
    "3. False Positives and False Negatives: Understanding the consequences of false positives and false negatives is essential in medical applications. Misclassifying a cancer patient as non-cancer (false negative) or a non-cancer patient as having cancer (false positive) can have severe implications and impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33212f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans5."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c33a271",
   "metadata": {},
   "source": [
    "A time series regression model can achieve higher accuracy than a decision tree on time series data because it's designed to understand patterns that occur over time. Time series models consider the order and timing of data points, making them more suitable for tasks like predicting future values in a sequence. Decision trees, on the other hand, may struggle with these time-related patterns and could be outperformed when dealing with time-dependent data.\n",
    "\n",
    "Decision trees may struggle with time series data due to their inability to handle temporal patterns directly. They typically make splits based on feature values without considering the sequence of observations.\n",
    "\n",
    "Decision trees can be prone to overfitting, especially in the presence of when the dataset is not sufficiently large. Time series regression models, when properly tuned, can offer a more robust representation of underlying patterns without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans6."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7cfa956",
   "metadata": {},
   "source": [
    "As the model is suffering from low bias and high variance it means the model is fitting the training data too closely but doesn't generalize well to new, unseen data. To address this we can use technique L1 or L2 regularization which can penalize large coefficients in the model, helping to prevent overfitting and reducing variance. With that Ensemble methods, such as Random Forests or Gradient Boosting, are effective in reducing variance. They build multiple models and combine their predictions, leading to a more stable and less overfit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans7."
   ]
  },
  {
   "cell_type": "raw",
   "id": "39d0eefc",
   "metadata": {},
   "source": [
    " \n",
    "Yes, it is  advisable to address highly correlated variables before applying Principal Component Analysis(PCA), because if the original variables are already highly correlated, PCA may not perform optimally because it assumes independence among the variables.\n",
    "Multicollinearity can inflate the importance of certain variables and make the interpretation of principal components less straightforward. It can also result in principal components that don't effectively capture unique variance.\n",
    "By addressing correlated variables before PCA, you ensure a more effective dimensionality reduction and improve the interpretability of the results, leading to a more robust and meaningful analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans8. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "bedb103a",
   "metadata": {},
   "source": [
    "As informed by the manager about multicollinearity, so to check for multicollinearity in a regression model we shall calculate Variance Inflation Factor (VIF). VIF measures how much the variance of an estimated regression coefficient increases if the predictors are correlated. High VIF values (mostly above 10) indicate multicollinearity. And then by using the correlation matrix among predictors. High correlation coefficients (close to 1 or -1) suggest multicollinearity.\n",
    "Also to plot a heatmap for better visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "An9."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0a13812",
   "metadata": {},
   "source": [
    "Ridge regression is particularly favorable when there is a high degree of multicollinearity among the predictor variables.\n",
    "It is suitable when you have a large number of predictors, and you want to shrink their coefficients without necessarily eliminating any of them.\n",
    "Ridge regression adds a penalty term (L2 regularization) to the linear regression cost function. The penalty term is the sum of the squared values of the coefficients multiplied by a regularization parameter (alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans10."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92468a89",
   "metadata": {},
   "source": [
    "Tree splitting is the process of dividing the data into smaller and more homogeneous groups based on a set of rules or criteria. The goal is to create a tree structure that can accurately classify the data into two classes.\n",
    "\n",
    "The tree splitting starts from the root node, which contains the entire data set. The algorithm chooses a variable and a split point that can best separate the data into two child nodes, according to a certain measure of impurity or information gain. The impurity measures how mixed the classes are in a node, and the information gain measures how much the split reduces the impurity. Some common impurity measures are Gini index, entropy, and misclassification error.\n",
    "\n",
    "The algorithm then repeats the same process for each child node, until a stopping criterion is met. The stopping criterion can be based on the depth of the tree, the number of observations in a node, the purity of a node, or the improvement of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ac9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans11."
   ]
  },
  {
   "cell_type": "raw",
   "id": "43ebacc8",
   "metadata": {},
   "source": [
    "Ordinary Least Squares (OLS) is a commonly used method in statistics and machine learning for estimating the parameters in a linear regression model. It's particularly suitable when working with a dataset having \n",
    "p variables and n observations.\n",
    "\n",
    "\n",
    "The number of variables (p) is large compared to the number of observations (n). This can cause overfitting, multicollinearity, and high variance in the estimates. The variables are not independent and identically distributed (i.i.d.). This can violate the assumptions of OLS and lead to biased and inconsistent estimates. The relationship between the variables and the response is not linear. This can reduce the accuracy and interpretability of the model.\n",
    "\n",
    "Some techniques that are better than OLS in these situations are:\n",
    "\n",
    "-Regularization methods: These methods add a penalty term to the cost function of OLS, which reduces the complexity and magnitude of the coefficients. Regularization methods can help to prevent overfitting, reduce multicollinearity, and improve the generalization ability of the model. Some common regularization methods are L1 (Lasso), L2 (Ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a462a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n"
     ]
    }
   ],
   "source": [
    "def print_dict(number):\n",
    "    if 1 <= number <= 20:\n",
    "        d = {i: i**2 for i in range(1, number + 1)}\n",
    "        print(d)\n",
    "    else:\n",
    "        print(\"The number must be between 1 and 20\")\n",
    "\n",
    "# Test the function with some examples\n",
    "print_dict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a858b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856d36b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5\n",
      "6 7 8 9 10\n"
     ]
    }
   ],
   "source": [
    "t = (1,2,3,4,5,6,7,8,9,10)\n",
    "t1 = t[:5]\n",
    "t2 = t[5:]\n",
    "print(*t1)\n",
    "print(*t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3db2c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106, 178, 134, 136, 140]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "even_numbers = [i for i in range(100, 201) if i % 2 == 0]\n",
    "random_list = random.sample(even_numbers, 5)\n",
    "print(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267aa097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yash's gender is Male\n",
      "Saniya's gender is Female\n"
     ]
    }
   ],
   "source": [
    " class Person:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def getGender(self):\n",
    "        return \"Unknown Gender\"\n",
    "\n",
    "class Male(Person):\n",
    "    def getGender(self):\n",
    "        return \"Male\"\n",
    "\n",
    "class Female(Person):\n",
    "    def getGender(self):\n",
    "        return \"Female\"\n",
    "\n",
    "# Example usage:\n",
    "person1 = Male(\"Yash\")\n",
    "person2 = Female(\"Saniya\")\n",
    "\n",
    "print(f\"{person1.name}'s gender is {person1.getGender()}\")\n",
    "print(f\"{person2.name}'s gender is {person2.getGender()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae76d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
